[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homework 1",
    "section": "",
    "text": "Jaryt Salvo\nDate: 9/21/24\nFall 2024 | CS7300 Unsupervised Learning\n\nThis project contains solutions to Homework 2 from the Unsupervised Learning course (CS7300) using Clojure. The primary purpose is to answer the given questions and demonstrate understanding of the concepts. The homework consists of three main questions:\n\nClassification with Nearest Neighbor (KNN): Implementing and evaluating KNN on the Iris dataset, including data splitting, elbow method for K selection, and exploration of different distance metrics.\nCross Validation for Ridge Regression: Performing Ridge regression with K-fold cross-validation on the Boston Housing dataset, including data loading, model fitting, and feature importance analysis.\nWriting Questions: Discussing the curse of dimensionality, comparing Ridge and Lasso regression, and providing insights on direct and iterative optimization methods.\n\nThe code is organized into different sections corresponding to each homework problem, with detailed explanations of the algorithms and mathematical concepts involved. We utilize Clojure and its associated libraries, such as scicloj.clay for rendering, tablecloth for data manipulation, and fastmath for mathematical operations.\n\n\nKey Features of the Implementation:\n\nKNN Classification (Q1):\n\nData loading and splitting (70/20/10 for train/validation/test)\nImplementation of the elbow method for optimal K selection\nExploration of different distance metrics (Euclidean, Manhattan, Cosine)\nModel evaluation and performance analysis\n\nRidge Regression (Q2):\n\nUse of scikit-learn via libpython-clj for model implementation\nK-fold cross-validation for hyperparameter tuning\nFeature importance analysis and visualization\nModel interpretation and equation formulation\n\nTheoretical Concepts (Q3):\n\nIn-depth discussion of the curse of dimensionality and mitigation strategies\nComparative analysis of Ridge and Lasso regularization techniques\nInsights into direct and iterative optimization methods\n\n\n\n\nTechnologies and Libraries Used:\n\nClojure as the primary programming language\nlibpython-clj for Python interoperability (scikit-learn)\ntablecloth for data manipulation\nfastmath for statistical computations\nscicloj.clay for notebook rendering\nHanamicloth for data visualization\n\nThis project demonstrates the application of machine learning concepts using Clojure, showcasing both practical implementations and theoretical understanding of key algorithms in unsupervised learning.\nThe code in the src/assignments folder was rendered with Clay and deployed with Github Pages."
  },
  {
    "objectID": "assignments.hw2.q1.html",
    "href": "assignments.hw2.q1.html",
    "title": "Question 1",
    "section": "",
    "text": "(ns assignments.hw2.q1\n  (:require\n   [my-py-clj.config :refer :all]\n   [assignments.hw2.utils :refer :all]\n   [fastmath.stats :as stats]\n  ;;  [libpython-clj2.python :refer [py.-]]\n  ;;  [scicloj.sklearn-clj.metamorph :as sklearn-mm]\n   [scicloj.hanamicloth.v1.api :as haclo]\n   [scicloj.metamorph.core :as morph]\n   [scicloj.metamorph.ml :as mm]\n   [scicloj.metamorph.ml.classification :as mlc]\n   [scicloj.metamorph.ml.gridsearch :as grid]\n   [scicloj.metamorph.ml.loss :as loss]\n   [scicloj.sklearn-clj :as sklearn-clj]\n   [scicloj.sklearn-clj.ml]                                ;; registers all models\n   [tablecloth.api :as tc]\n   [tech.v3.dataset.metamorph :as dsm]\n   [tech.v3.dataset.modelling :as ds-mod]))\n\n\n\nQ1: Classification with Nearest Neighbor (30 Points)\nFor this question, you will need to perform KNN on the famous Iris data set. You are required to use Scki-learn to completion this question. The data is stored in a csv file and it can be downloaded from Canvas. For the description of the data set, you can visit: Wikipedia iris dataset.\n1) Load the data and split it into train, valid, and test (70/20/10).\n\n(defonce iris (-&gt; \"data/A1Q1_Data.csv\"\n                  (tc/dataset {:key-fn (fn [colname]\n                                         (-&gt; colname    ;kabab-case keyword\n                                             (clojure.string/replace #\"\\.|\\s\" \"-\")\n                                             clojure.string/lower-case\n                                             keyword))})\n                  (ds-mod/set-inference-target :variety)))\n\n\n(def response :variety)\n\n\n(def regressors\n  (tc/column-names iris (complement #{response})))\n\n\n(let [data (-&gt; iris\n               (tc/split-&gt;seq :holdout {:seed 123 :ratio 0.9}))\n      test-data (-&gt; data first :test)\n      train-val-data (-&gt; data first :train\n                         (tc/split-&gt;seq :holdout {:seed 123}))\n      train-data (-&gt; train-val-data first :train)\n      val-data (-&gt; train-val-data first :test)]\n  (def test-data test-data)\n  (def train-data train-data)\n  (def val-data val-data))\n\n\n#'assignments.hw2.q1/val-data\n\ntc/split-&gt;seq is a function that splits a dataset into two or more subsets. In this case, it’s dividing the dataset into a test set and a training set. The :holdout option specifies that we want to split the dataset into two subsets, while the :ratio option determines the proportion of the dataset to include in the training set.\nWith this 90/10 split, we can further divide the training set into training and validation sets to tune our hyperparameters. The test set is already accessible in the data variable by calling first, which indicates the first element (map) of the sequence. The :test key in this map represents the 10% of the data set aside for testing, as specified in the tc/split-&gt;seq function call.\n2) Write a function that uses the elbow method to select the value for K. You can set the range for K as (1, 15).\n\n(defn create-pipeline [params]\n  (morph/pipeline\n   (dsm/categorical-&gt;number [response])\n   (dsm/set-inference-target response)\n   {:metamorph/id :model}\n   (mm/model (merge {:model-type :sklearn.classification/k-neighbors-classifier}\n                    params))))\n\nThis function creates a pipeline for the KNN model, converting categorical data to numbers, setting the inference target, and creating the model with given parameters.\n\n(defn generate-hyperparams []\n  (grid/sobol-gridsearch\n   {:n-neighbors (grid/linear 1 15 15 :int32)\n    :weights     (grid/categorical [\"distance\"])\n    :metric      (grid/categorical [\"euclidean\" \"manhattan\" \"cosine\"])}))\n\nThis function generates hyperparameters for the KNN model using Sobol sequence for efficient space exploration, including neighbors (1-15), weights, and distance metrics.\n\n(grid/sobol-gridsearch\n {:n-neighbors (grid/linear 1 2 2 :int32)\n  :metric      (grid/categorical [\"euclidean\" \"manhattan\"])})\n\n\n({:n-neighbors 2, :metric \"manhattan\"}\n {:n-neighbors 2, :metric \"euclidean\"}\n {:n-neighbors 1, :metric \"manhattan\"}\n {:n-neighbors 1, :metric \"euclidean\"})\n\n\n(defn evaluate-model [pipelines data-seq]\n  (mm/evaluate-pipelines\n   pipelines\n   data-seq\n   stats/cohens-kappa\n   :accuracy\n   {:other-metrices\n    [{:name :mathews-cor-coef :metric-fn stats/mcc}\n     {:name :accuracy :metric-fn loss/classification-accuracy}]\n    :return-best-pipeline-only        false\n    :return-best-crossvalidation-only true}))\n\nThis function evaluates the model pipelines using various metrics like Cohen’s kappa, Matthews correlation coefficient, and accuracy.\n\n(defn process-results [evaluations]\n  (-&gt;&gt; evaluations\n       flatten\n       (map #(hash-map\n              :summary (mm/thaw-model (get-in % [:fit-ctx :model]))\n              :fit-ctx (:fit-ctx %)\n              :timing-fit (:timing-fit %)\n              :metric ((comp :metric :test-transform) %)\n              :other-metrices ((comp :other-metrices :test-transform) %)\n              :params ((comp :options :model :fit-ctx) %)\n              :lookup-table (get-in % [:fit-ctx :model :target-categorical-maps :variety :lookup-table])\n              :pipe-fn (:pipe-fn %)))\n       (sort-by :metric)\n       reverse))\n\nThis function processes the evaluation results, extracting relevant information and sorting the results by metric score in descending order.\n\n(defn elbow-method [train-data val-data]\n  (let [pipelines (map create-pipeline (generate-hyperparams))\n        evaluations (evaluate-model pipelines [{:train train-data :test val-data}])]\n    (process-results evaluations)))\n\nThis function implements the elbow method by creating pipelines with different hyperparameters, evaluating them, and processing the results to find the optimal K value.\n\n(def elbow-results\n  (elbow-method train-data val-data))\n\nThis line applies the elbow method to the training and validation data, storing the results as elbow-results.\n\n(count elbow-results)\n\n\n45\n\n\n(def results-dataset\n  (-&gt;&gt; elbow-results\n       (map (fn [result]\n              (let [k (get-in result [:params :n-neighbors])\n                    dist-metric (get-in result [:params :metric])\n                    kappa (:metric result)\n                    other-metrics (:other-metrices result)\n                    mcc (-&gt; (filter #(= (:name %) :mathews-cor-coef) other-metrics)\n                            first\n                            :metric)\n                    accuracy (-&gt; (filter #(= (:name %) :accuracy) other-metrics)\n                                 first\n                                 :metric)]\n                {:k        k\n                 :metric   dist-metric\n                 :kappa    kappa\n                 :mcc      mcc\n                 :accuracy accuracy})))\n       (tc/dataset)))\n\nThis code processes the elbow results, extracting key metrics (k, distance metric, kappa, MCC, accuracy) and creates a dataset for easier analysis and visualization.\n\n(let [data (tc/select-rows results-dataset (comp #(= % \"euclidean\") :metric))]\n  (-&gt; data\n      (haclo/layer-line {:=x :k :=y :kappa})\n      (haclo/layer-point {:=x         :k :=y :kappa\n                          :=mark-size 50})))\n\n\nThis code creates a plot of the elbow method results for the Euclidean distance metric, showing how kappa changes with different K values.\n\nLooking at our elbow plot, we can see that the elbow dips down at \\(K=8\\). Therefore, we want a K before 8. To me, 1 is too small and even numbers won’t necessarily have a majority in a vote. I’d say good choices are 3, 5, or 7. I’ll choose 5 because it’s the middle value.\n\n3) Explore different distance metrics and repeat part 2 with another distance metric. In the report, justify the value of K and the distance metric.\n\n(let [data (tc/select-rows results-dataset (comp #(= % \"manhattan\") :metric))]\n  (-&gt; data\n      (haclo/layer-line {:=x :k :=y :mcc})\n      (haclo/layer-point {:=x         :k :=y :mcc\n                          :=mark-size 50})))\n\n\n\n(let [data (tc/select-rows results-dataset (comp #(= % \"cosine\") :metric))]\n  (-&gt; data\n      (haclo/layer-line {:=x :k :=y :mcc})\n      (haclo/layer-point {:=x         :k :=y :mcc\n                          :=mark-size 50})))\n\n\nJustification for K value and distance metric:\n1. Euclidean distance (default):\n\nOptimal K: \\([3, 5, 7]\\)\nThis metric is suitable for continuous features and assumes all features contribute equally.\nIt works well when the relationship between features is linear.\n\n2. Manhattan distance:\n\nOptimal K: \\([3, 5]\\)\nThis metric is less sensitive to outliers compared to Euclidean distance.\nIt’s particularly useful when features are on different scales or when dealing with high-dimensional data.\n\n3. Cosine distance:\n\nOptimal K: \\([6, 10, 12, 14, 15]\\)\nThis metric is useful for high-dimensional data where feature scaling is important.\nIt’s particularly useful when the angle between data points is more important than their magnitude.\n\nThe choice between these metrics depends on the specific characteristics of the Iris dataset:\n\nIf the features are on similar scales and have a roughly linear relationship, Euclidean distance might be preferred.\nIf there are potential outliers or the features are on different scales, Manhattan distance could be more appropriate.\nIf the data is high-dimensional and feature scaling is important, cosine distance might be the best choice.\n\n\nThe optimal K value balances between overfitting (low K) and underfitting (high K). The choice of distance metric depends on the specific characteristics of the dataset. Euclidean distance is a fine choice for this dataset for reasons explained above as well as being the easiest to understand. As such, I’d probably choose \\(K=5\\) using the Euclidean distance metric.\n\n4) Train the KNN model with the optimal K value and chosen distance metric on the combined train and validation sets.\n\n(def train-val-data-bootstrapped\n  (-&gt; train-data\n      (tc/concat val-data)\n      (tc/split-&gt;seq :bootstrap {:seed 123 :repeats 25})))\n\nThis code combines the training and validation data, then creates 25 bootstrap samples for robust model evaluation.\n\n(def final-model\n  (let [pipelines (map create-pipeline [{:n-neighbors 5\n                                         :weights     \"distance\"\n                                         :metric      \"euclidean\"}])\n        evaluations (evaluate-model pipelines train-val-data-bootstrapped)]\n    (process-results evaluations)))\n\nHere we define the final model using the chosen hyperparameters (5 neighbors, Euclidean distance) and evaluate it on the bootstrapped data.\n\n(-&gt; final-model first :lookup-table)\n\n\n{\"Versicolor\" 0, \"Setosa\" 1, \"Virginica\" 2}\n\nThis line retrieves the lookup table from the final model, which maps categorical labels to numerical values.\n5) Evaluate the model on the test set and report the accuracy.\n\n(defn actual\n  [model]\n  (-&gt; test-data :variety vec))\n\n\n(defn preds\n  [model]\n  (let [lookup-table (-&gt; model first :lookup-table)\n        reverse-lookup (zipmap (vals lookup-table) (keys lookup-table))]\n    (-&gt; test-data\n        (morph/transform-pipe\n         (-&gt; model first :pipe-fn)\n         (-&gt; model first :fit-ctx))\n        :metamorph/data\n        :variety\n        (-&gt;&gt; (map #(get reverse-lookup (long %)))\n             vec))))\n\nThese functions extract the actual labels from the test data and generate predictions using the trained model, respectively.\n\n(defn evaluate-predictions\n  \"Evaluates predictions against actual labels, returns confusion map and metrics.\"\n  [preds actual]\n  (let [conf-map (mlc/confusion-map-&gt;ds (mlc/confusion-map preds actual :none))\n        accuracy (loss/classification-accuracy preds actual)\n        kappa (stats/cohens-kappa preds actual)\n        mcc (stats/mcc preds actual)]\n    {:confusion-map conf-map\n     :accuracy      (format \"%.4f\" accuracy)\n     :cohens-kappa  (format \"%.4f\" kappa)\n     :mcc           (format \"%.4f\" mcc)}))\n\nevaluate-predictions calculates various performance metrics including accuracy, Cohen’s kappa, and Matthews correlation coefficient, along with a confusion matrix. At last, we generate predictions on the test set, compare them with the actual labels, and compute the evaluation metrics to assess the model’s performance.\n\n(let [preds (preds final-model)\n      actual (actual final-model)]\n  (evaluate-predictions preds actual))\n\n{\n\n\n\n\n\n\n\n\n:confusion-map\n\n\n\n_unnamed [3 4]:\n\n\n\n:column-name\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n5\n0\n0\n\n\nVersicolor\n0\n5\n0\n\n\nVirginica\n0\n1\n4\n\n\n\n\n\n\n\n\n:accuracy \"0.9333\":cohens-kappa \"0.9000\":mcc \"0.9061\"}\n\n(answer (str \"Accuracy on the Iris test set: \"\n             (get-in (let [preds (preds final-model)\n                           actual (actual final-model)]\n                       (evaluate-predictions preds actual))\n                     [:accuracy])))\n\n\nAccuracy on the Iris test set: 0.9333\n\n6) Provide a brief analysis of the results in your report.\n\n\nAnalysis of Results:\n\n1. Data Split:\nWe used a 70/20/10 split for train/validation/test, which is a common practice. This split provides enough data for training while reserving sufficient data for validation and testing. However, the Iris dataset is small, where the 10% holdout is just 15 samples.\n\n\n2. Elbow Method:\nThis approach effectively determined the optimal K value, balancing between model complexity and performance, between underfitting and overfitting.\n\n\n3. Distance Metrics:\nComparison of Euclidean and Manhattan distances revealed metric-dependent optimal K values, underscoring the importance of metric selection in KNN.\n\n\n4. Final Model:\nWe selected Euclidean distance with K = 5, based on the elbow method results. This choice aims to balance model simplicity with performance.\n\n\n5. Model Performance:\nThe final accuracy of \\(0.9333\\) on the test set demonstrates strong generalization to unseen data, validating our K and distance metric choices.\n\n\n6. Limitations and Future Work:\n\nExplore additional distance metrics and feature scaling techniques for potential performance improvements.\nImplement k-fold cross-validation for more robust performance estimation.\nConduct feature importance analysis to identify key iris characteristics for classification.\nConsider testing the model on a larger, more diverse dataset to assess its broader applicability."
  },
  {
    "objectID": "assignments.hw2.q2.html",
    "href": "assignments.hw2.q2.html",
    "title": "Question 2",
    "section": "",
    "text": "(ns assignments.hw2.q2\n  (:require\n   [assignments.hw2.utils :refer :all]\n   ;; [scicloj.sklearn-clj]\n   [scicloj.hanamicloth.v1.api :as haclo]\n   [libpython-clj2.python :refer [py. py.-]]\n   [libpython-clj2.require :refer [require-python]]\n   [my-py-clj.config :refer :all]\n   [tablecloth.api :as tc]\n   [scicloj.kindly.v4.kind :as kind]))\n\n\n\nQ2: Cross Validation for Ridge Regression (40 points)\nFor this question, you will need to perform Ridge (\\(L^2\\)) regression with K-fold cross validation via Scki-Learn. The data set is the famous Boston Housing data, which is a benchmark data for regression. The data contains 14 attributes and the medv, median value of owner-occupied homes in $1000s, will be your target to predict. The data set can be downloaded from canvas. You can also find some helpful information at Kaggle.\nIn lecture, we learned training, test, and validation. However, in the real-world, we cannot always afford to implement it due to insufficient data. An alternative solution is K-fold cross validation which uses a part of the available data to fit the model, and a different part to test it. K-fold CV procedure splits the data into K equal-sized parts.\n1) Load the train data and test data\n\n(require-python '[numpy :as np])\n\n\n:ok\n\n\n(require-python '[pandas :as pd])\n\n\n:ok\n\n\n(require-python '[sklearn.model_selection :as model-selection])\n\n\n:ok\n\n\n(require-python '[sklearn.linear_model :as linear-model])\n\n\n:ok\n\n\n(require-python '[sklearn.metrics :as metrics])\n\n\n:ok\n\nThese lines import necessary Python libraries for data manipulation, model selection, and evaluation. I wish I hide the :ok code-outputs, alas, I haven’t found that option yet.\n\n(def train-data (pd/read_csv \"data/A1Q2_Train_Data.csv\"))\n\n\n(def test-data (pd/read_csv \"data/A1Q2_Test_Data.csv\"))\n\nHere we load the training and test datasets from CSV files using pandas.\n\n(pd/DataFrame train-data)\n\n\n      ID     crim    zn  indus  chas  ...  tax  ptratio   black  lstat  medv\n0      1  0.00632  18.0   2.31     0  ...  296     15.3  396.90   4.98  24.0\n1      2  0.02731   0.0   7.07     0  ...  242     17.8  396.90   9.14  21.6\n2      4  0.03237   0.0   2.18     0  ...  222     18.7  394.63   2.94  33.4\n3      5  0.06905   0.0   2.18     0  ...  222     18.7  396.90   5.33  36.2\n4      7  0.08829  12.5   7.87     0  ...  311     15.2  395.60  12.43  22.9\n..   ...      ...   ...    ...   ...  ...  ...      ...     ...    ...   ...\n328  500  0.17783   0.0   9.69     0  ...  391     19.2  395.77  15.10  17.5\n329  502  0.06263   0.0  11.93     0  ...  273     21.0  391.99   9.67  22.4\n330  503  0.04527   0.0  11.93     0  ...  273     21.0  396.90   9.08  20.6\n331  504  0.06076   0.0  11.93     0  ...  273     21.0  396.90   5.64  23.9\n332  506  0.04741   0.0  11.93     0  ...  273     21.0  396.90   7.88  11.9\n\n[333 rows x 15 columns]\n\n\n(pd/DataFrame test-data)\n\n\n      ID     crim    zn  indus  chas  ...  rad  tax  ptratio   black  lstat\n0      3  0.02729   0.0   7.07     0  ...    2  242     17.8  392.83   4.03\n1      6  0.02985   0.0   2.18     0  ...    3  222     18.7  394.12   5.21\n2      8  0.14455  12.5   7.87     0  ...    5  311     15.2  396.90  19.15\n3      9  0.21124  12.5   7.87     0  ...    5  311     15.2  386.63  29.93\n4     10  0.17004  12.5   7.87     0  ...    5  311     15.2  386.71  17.10\n..   ...      ...   ...    ...   ...  ...  ...  ...      ...     ...    ...\n168  496  0.17899   0.0   9.69     0  ...    6  391     19.2  393.29  17.60\n169  497  0.28960   0.0   9.69     0  ...    6  391     19.2  396.90  21.14\n170  499  0.23912   0.0   9.69     0  ...    6  391     19.2  396.90  12.92\n171  501  0.22438   0.0   9.69     0  ...    6  391     19.2  396.90  14.33\n172  505  0.10959   0.0  11.93     0  ...    1  273     21.0  393.45   6.48\n\n[173 rows x 14 columns]\n\nInspect the loaded datasets as pandas DataFrames.\n\n(def X-train (py. train-data drop \"medv\" :axis 1))\n\n\n(def y-train (py. train-data \"get\" \"medv\"))\n\nWe separate the features (X-train) and target variable (y-train) from the training data.\n2) Perform Ridge regression on the train data\n\n(def ridge-model (linear-model/Ridge))\n\n\n(def param-grid {\"alpha\" [150 160 165 170 175 180 185 190 195 200]})\n\n\n(def cv-ridge (model-selection/GridSearchCV ridge-model param-grid :cv 5))\n\nHere we set up the Ridge regression model and define a parameter grid for alpha values. We then create a GridSearchCV object for 5-fold cross-validation.\n\n(py. cv-ridge fit X-train y-train)\n\n\nGridSearchCV(cv=5, estimator=Ridge(),\n             param_grid={'alpha': [150, 160, 165, 170, 175, 180, 185, 190, 195,\n                                   200]})\n\nThis line fits the Ridge regression model using GridSearchCV on the training data.\n\n(answer (str \"Best parameters:\" (py.- cv-ridge best_params_)))\n\n\nBest parameters:{‘alpha’: 180}\n\n\n(answer (str \"Best CV score:\" (py.- cv-ridge best_score_)))\n\n\nBest CV score:0.17820256573782292\n\n\n\nK-fold Cross Validation Procedure:\n\nThe training data is divided into 5 equal parts (folds).\nFor each fold:\n\nThat fold is treated as a validation set.\nThe model is trained on the remaining 4 folds.\nThe model’s performance is evaluated on the validation fold.\n\nThis process is repeated 5 times, with each fold serving as the validation set once.\nThe average performance across all 5 validations is used as the cross-validation score.\nThis entire procedure is repeated for each hyperparameter combination (different alpha values).\nThe hyperparameters that yield the best average performance are selected.\n\nThis method provides a robust estimate of the model’s performance and helps in selecting the best hyperparameters, reducing the risk of overfitting.\nBelow, we print the best model’s intercept and coefficients, and construct a human-readable equation for the Ridge regression model.\n\n(let [best-model (py.- cv-ridge best_estimator_)\n      intercept (py.- best-model intercept_)\n      coefficients (py.- best-model coef_)\n      feature-names (py.- X-train columns)]\n  (answer\n   (str \"Ridge Regression Equation:\\n\"\n        \"$medv = \"\n        (format \"%.4f\" intercept)\n        (apply str\n               (map (fn [name coef]\n                      (format \" %s %.4f * %s\"\n                              (if (pos? coef) \"+\" \"-\")\n                              (Math/abs coef)\n                              name))\n                    feature-names\n                    coefficients)) \"$\")))\n\n\nRidge Regression Equation: \\(medv = 39.8963 - 0.0023 * ID - 0.0490 * crim + 0.0578 * zn + 0.0043 * indus + 0.3847 * chas - 0.0680 * nox + 1.3832 * rm + 0.0129 * age - 0.9845 * dis + 0.3697 * rad - 0.0167 * tax - 0.7580 * ptratio + 0.0116 * black - 0.7889 * lstat\\)\n\n3) Justify the choice of K\nWe chose \\(K=5\\) for cross-validation as it provides a good balance between bias and variance. It’s a common choice that works well for most datasets, offering reliable performance estimates without excessive computational cost.\n4) Test the model on the test data\n\n(def best-ridge-model (py.- cv-ridge best_estimator_))\n\nHere we extract the best Ridge regression model from the GridSearchCV results.\n\n(def y-pred (py. best-ridge-model predict test-data))\n\ny-pred is the predictions on the entire test dataset using the best Ridge regression model.\n5) Analyze the importance of each feature and justify your results in the report\n\n(def feature-importance (py.- best-ridge-model coef_))\n\n\n(def feature-names (py.- X-train columns))\n\nThese lines extract the feature coefficients (which represent feature importance in linear models) and feature names from the best model and training data, respectively.\n\n(let [feature-names (vec feature-names)\n      feature-importance (vec feature-importance)\n      data (tc/dataset {:vars feature-names\n                        :importance feature-importance})\n      sorted (tc/order-by data :importance :desc)]\n  (-&gt; sorted\n      (haclo/layer-bar\n       {:=y :vars :=x :importance       ;order-by??\n        :=title \"Feature Importances\"})))\n\n\nThe barplot shows the regressor coefficients, which are proportional to the feature importances. The plot is generated using the Hanami plotting library.\n\n(answer\n (str \"Feature importances:\\n\"\n      (clojure.string/join \"\\n\"\n                           (for [[feature importance] (map vector feature-names feature-importance)]\n                             (format \"%-20s : %.4f ;  \" feature importance)))))\n\n\nFeature importances: ID : -0.0023 ;\ncrim : -0.0490 ;\nzn : 0.0578 ;\nindus : 0.0043 ;\nchas : 0.3847 ;\nnox : -0.0680 ;\nrm : 1.3832 ;\nage : 0.0129 ;\ndis : -0.9845 ;\nrad : 0.3697 ;\ntax : -0.0167 ;\nptratio : -0.7580 ;\nblack : 0.0116 ;\nlstat : -0.7889 ; \n\nThis generates a formatted string output of all feature importances, aligning feature names and their corresponding importance values.\nSort features by absolute importance\n\n(def sorted-features\n  (-&gt;&gt; (map vector feature-names feature-importance)\n       (sort-by #(Math/abs (second %)))\n       reverse))\n\nThis code sorts the features by the absolute value of their importance (coefficient magnitude) in descending order. This is useful because both large positive and large negative coefficients indicate important features in linear models.\nFinally, this code outputs a formatted string of the top 5 most important features based on the absolute value of their coefficients. This provides a quick summary of which features have the largest impact on the model’s predictions.\n\n(answer\n (str \"\\nTop 5 most important features:\\n\"\n      (clojure.string/join \"\\n\"\n                           (for [[feature importance] (take 5 sorted-features)]\n                             (format \"%-20s : %.4f ;  \" feature (float importance))))))\n\n\n Top 5 most important features: rm : 1.3832 ;\ndis : -0.9845 ;\nlstat : -0.7889 ;\nptratio : -0.7580 ;\nchas : 0.3847 ; \n\n\n\nExplanation of feature importances:\n\nThe most important features are those with the largest absolute coefficient values.\nPositive coefficients indicate that an increase in the feature leads to an increase in the predicted house price, while negative coefficients indicate the opposite.\nThe magnitude of the coefficient represents the feature’s relative importance in predicting the house price.\nFeatures with near-zero coefficients have little impact on the prediction.\n\nThis analysis helps us understand which factors most strongly influence house prices in the Boston housing dataset."
  },
  {
    "objectID": "assignments.hw2.q3.html",
    "href": "assignments.hw2.q3.html",
    "title": "Question 3",
    "section": "",
    "text": "(ns assignments.hw2.q3\n  (:require\n   [assignments.hw2.utils :refer :all]\n   [tablecloth.api :as tc]\n   [fastmath.core :as m]\n   [fastmath.stats :as s]))\n\n\n\nQ3: Writing Questions (20 points)\n1) What is the curse of dimensionality and list three ways to avoid it. (6 points)\n\n\nCurse of Dimensionality\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. As the number of features or dimensions grows, the amount of data needed to generalize accurately grows exponentially. This curse makes it challenging for machine learning algorithms to be effective in high dimensions.\n\nKey aspects of the curse of dimensionality:\n\n1. Sparsity:\nData becomes sparse in high dimensions, making it difficult to find patterns.\n\n\n2. Distance Concentration:\nDistances between points become less meaningful as dimensions increase.\n\n\n3. Model Complexity:\nThe number of parameters in models often increases exponentially with dimensions.\n\n\n\nThree ways to mitigate the curse of dimensionality:\n\n1. Dimensionality Reduction:\n\nUse techniques like PCA, t-SNE, or autoencoders to reduce the number of features while preserving important information.\nThis helps in visualizing high-dimensional data and can improve model performance.\n\n\n\n2. Feature Selection:\n\nChoose only the most relevant features for your model using methods like Lasso, Ridge regression, or tree-based feature importance.\nThis reduces noise and helps focus on the most informative aspects of the data.\n\n\n\n3. Regularization:\n\nApply techniques like \\(L^1\\) (Lasso) or \\(L^2\\) (Ridge) regularization to prevent overfitting in high-dimensional spaces.\nThis constrains model complexity and can lead to better generalization.\n\n\n\n\nAdditional Strategies\nWe could also collect more data, use domain knowledge to guide feature engineering, and employ algorithms that are less sensitive to high dimensionality, such as decision trees or random forests.\n2) Explain the differences between Ridge penalty and Lasso penalty. (6 points)\n\n\n\nRidge vs. Lasso\nRidge (\\(L^2\\)) and Lasso (\\(L^1\\)) penalties are both regularization techniques used in linear models to prevent overfitting, but they differ in their approach and effects:\n\n1. Mathematical Formulation:\n\nRidge: Adds the sum of squared coefficients (L2 norm) to the loss function. Penalty term: \\[λ_2 * Σ(β_i^2)\\]\nLasso: Adds the sum of absolute values of coefficients (L1 norm) to the loss function. Penalty term: \\[λ_1 * Σ|β_i|\\]\n\n\n\n2. Effect on Coefficients:\n\nRidge: Shrinks all coefficients towards zero, but rarely makes them exactly zero.\nLasso: Can shrink coefficients to exactly zero, effectively performing feature selection.\n\n\n\n3. Geometry:\n\nRidge: Creates a circular (in 2D) or spherical (in higher dimensions) constraint region.\nLasso: Creates a diamond-shaped (in 2D) or pyramidal (in higher dimensions) constraint region.\n\n\n\n4. Solution Characteristics:\n\nRidge: Always has a unique solution due to the strictly convex nature of its objective function.\nLasso: May have multiple solutions when features are highly correlated.\n\n\n\n5. Feature Selection:\n\nRidge: Keeps all features but reduces their impact.\nLasso: Can completely eliminate less important features, leading to sparse models.\n\n\n\n6. Handling Correlated Features:\n\nRidge: Tends to shrink coefficients of correlated features towards each other.\nLasso: Tends to pick one of the correlated features and ignore the others.\n\nIn practice, the choice between Ridge and Lasso depends on the specific problem. Ridge is often preferred when all features are potentially relevant, while Lasso is useful when feature selection is desired or when dealing with high-dimensional data with many irrelevant features)\n3) Demonstrate your insights for both direct and iterative methods for optimization. (8 points)\nDirect and iterative methods are two fundamental approaches to optimization in machine learning and numerical computing:\n\n\n\nDirect and Iterative Methods:\n\nDirect Methods:\n\n1. Characteristics:\n\nAttempt to solve the problem in a finite number of steps.\nOften involve matrix operations or analytical solutions.\nExamples: Gaussian elimination, LU decomposition, closed-form solutions.\n\n\n\n2. Advantages and Disadvantages:\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nCan provide exact solutions for certain problems\nMay be computationally expensive or infeasible for large-scale problems\n\n\nOften faster for small to medium-sized problems\nCan be sensitive to numerical errors in floating-point arithmetic\n\n\nNo need to specify convergence criteria or iteration limits\nNot always available for complex, non-linear problems\n\n\n\n\n\n3. Use Cases:\n\nSolving systems of linear equations.\nFinding eigenvalues and eigenvectors.\nLeast squares problems with closed-form solutions.\n\n\n\n\nIterative Methods:\n\n1. Characteristics:\n\nStart with an initial guess and progressively refine the solution.\nInvolve repeated application of an algorithm until convergence.\nExamples: Gradient descent, Newton’s method, conjugate gradient.\n\n\n\n2. Advantages and Disadvantages:\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nCan handle large-scale and complex optimization problems\nMay require many iterations to converge\n\n\nOften more memory-efficient than direct methods\nConvergence is not always guaranteed and can depend on initial conditions\n\n\nCan provide approximate solutions quickly, even if not run to full convergence\nNeed to specify stopping criteria (e.g., tolerance, maximum iterations)\n\n\n\n\n\n3. Use Cases:\n\nTraining of neural networks and other machine learning models.\nSolving large sparse systems of equations.\nOptimization problems where direct methods are infeasible.\n\n\n\n\nInsights:\n\n1. Problem-Dependent Choice:\nThe choice between direct and iterative methods often depends on the specific problem, its size, and structure.\n\n\n2. Hybrid Approaches:\nSome advanced algorithms combine aspects of both, like using direct methods as preconditioners for iterative methods.\n\n\n3. Scalability Considerations:\nAs problem sizes grow, iterative methods often become the only feasible option due to memory and computational constraints.\n\n\n4. Accuracy vs. Speed Trade-off:\nDirect methods can provide highly accurate solutions but may be slow for large problems. Iterative methods can often provide good approximate solutions quickly.\n\n\n5. Robustness:\nIterative methods can be more robust to certain types of numerical errors and can handle ill-conditioned problems better in some cases.\n\n\n6. Adaptability:\nIterative methods are often more adaptable to changes in the problem structure or constraints during the optimization process.\nIn machine learning, iterative methods are prevalent due to the large-scale nature of many problems and the need for flexibility in handling various loss functions and model architectures. However, understanding both approaches is crucial for selecting the most appropriate method for a given optimization task"
  }
]