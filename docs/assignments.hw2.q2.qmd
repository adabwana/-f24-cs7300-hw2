
---
format:
  html: {toc: true, toc-depth: 3, theme: cosmo, number-sections: false, output-file: assignments.hw2.q2.html}
fontsize: 0.9em
code-block-background: true
include-in-header: {text: '<link rel = "icon" href = "data:," />'}
toc-title-numbers: false
number-depth: 0

---
<style></style><style>.printedClojure .sourceCode {
  background-color: transparent;
  border-style: none;
}
</style><style>.clay-limit-image-width .clay-image {max-width: 100%}
.clay-side-by-side .sourceCode {margin: 0}
.clay-side-by-side {margin: 1em 0}
</style>
<script src="assignments.hw2.q2_files/md-default1.js" type="text/javascript"></script><script src="assignments.hw2.q2_files/md-default2.js" type="text/javascript"></script><script src="assignments.hw2.q2_files/vega3.js" type="text/javascript"></script><script src="assignments.hw2.q2_files/vega4.js" type="text/javascript"></script><script src="assignments.hw2.q2_files/vega5.js" type="text/javascript"></script>

::: {.sourceClojure}
```clojure
(ns assignments.hw2.q2
  (:require
   [assignments.hw2.utils :refer :all]
   ;; [scicloj.sklearn-clj]
   [scicloj.hanamicloth.v1.api :as haclo]
   [libpython-clj2.python :refer [py. py.-]]
   [libpython-clj2.require :refer [require-python]]
   [my-py-clj.config :refer :all]
   [tablecloth.api :as tc]
   [scicloj.kindly.v4.kind :as kind]))
```
:::


## Question 2
---

#### *Q2: Cross Validation for Ridge Regression (40 points)*

For this question, you will need to perform Ridge ($L^2$) regression with K-fold cross validation via Scki-Learn. The data set is the famous Boston Housing data, which is a benchmark data for regression. The data contains 14 attributes and the `medv`, median value of owner-occupied homes in $1000s, will be your target to predict. The data set can be downloaded from canvas. You can also find some helpful information at [Kaggle](https://www.kaggle.com/code/henriqueyamahata/boston-housing-with-linear-regression/notebook). 
  
  In lecture, we learned training, test, and validation. However, in the real-world, we cannot always afford to implement it due to insufficient data. An alternative solution is K-fold cross validation which uses a part of the available data to fit the model, and a different part to test it. K-fold CV procedure splits the data into K equal-sized parts.

***1) Load the train data and test data***


::: {.sourceClojure}
```clojure
(require-python '[numpy :as np])
```
:::



::: {.printedClojure}
```clojure
:ok

```
:::



::: {.sourceClojure}
```clojure
(require-python '[pandas :as pd])
```
:::



::: {.printedClojure}
```clojure
:ok

```
:::



::: {.sourceClojure}
```clojure
(require-python '[sklearn.model_selection :as model-selection])
```
:::



::: {.printedClojure}
```clojure
:ok

```
:::



::: {.sourceClojure}
```clojure
(require-python '[sklearn.linear_model :as linear-model])
```
:::



::: {.printedClojure}
```clojure
:ok

```
:::



::: {.sourceClojure}
```clojure
(require-python '[sklearn.metrics :as metrics])
```
:::



::: {.printedClojure}
```clojure
:ok

```
:::


These lines import necessary Python libraries for data manipulation, model selection, and evaluation. I wish I hide the `:ok` code-outputs, alas, I haven't found that option yet.


::: {.sourceClojure}
```clojure
(def train-data (pd/read_csv "data/A1Q2_Train_Data.csv"))
```
:::



::: {.sourceClojure}
```clojure
(def test-data (pd/read_csv "data/A1Q2_Test_Data.csv"))
```
:::


Here we load the training and test datasets from CSV files using pandas.


::: {.sourceClojure}
```clojure
(pd/DataFrame train-data)
```
:::



::: {.printedClojure}
```clojure
      ID     crim    zn  indus  chas  ...  tax  ptratio   black  lstat  medv
0      1  0.00632  18.0   2.31     0  ...  296     15.3  396.90   4.98  24.0
1      2  0.02731   0.0   7.07     0  ...  242     17.8  396.90   9.14  21.6
2      4  0.03237   0.0   2.18     0  ...  222     18.7  394.63   2.94  33.4
3      5  0.06905   0.0   2.18     0  ...  222     18.7  396.90   5.33  36.2
4      7  0.08829  12.5   7.87     0  ...  311     15.2  395.60  12.43  22.9
..   ...      ...   ...    ...   ...  ...  ...      ...     ...    ...   ...
328  500  0.17783   0.0   9.69     0  ...  391     19.2  395.77  15.10  17.5
329  502  0.06263   0.0  11.93     0  ...  273     21.0  391.99   9.67  22.4
330  503  0.04527   0.0  11.93     0  ...  273     21.0  396.90   9.08  20.6
331  504  0.06076   0.0  11.93     0  ...  273     21.0  396.90   5.64  23.9
332  506  0.04741   0.0  11.93     0  ...  273     21.0  396.90   7.88  11.9

[333 rows x 15 columns]

```
:::



::: {.sourceClojure}
```clojure
(pd/DataFrame test-data)
```
:::



::: {.printedClojure}
```clojure
      ID     crim    zn  indus  chas  ...  rad  tax  ptratio   black  lstat
0      3  0.02729   0.0   7.07     0  ...    2  242     17.8  392.83   4.03
1      6  0.02985   0.0   2.18     0  ...    3  222     18.7  394.12   5.21
2      8  0.14455  12.5   7.87     0  ...    5  311     15.2  396.90  19.15
3      9  0.21124  12.5   7.87     0  ...    5  311     15.2  386.63  29.93
4     10  0.17004  12.5   7.87     0  ...    5  311     15.2  386.71  17.10
..   ...      ...   ...    ...   ...  ...  ...  ...      ...     ...    ...
168  496  0.17899   0.0   9.69     0  ...    6  391     19.2  393.29  17.60
169  497  0.28960   0.0   9.69     0  ...    6  391     19.2  396.90  21.14
170  499  0.23912   0.0   9.69     0  ...    6  391     19.2  396.90  12.92
171  501  0.22438   0.0   9.69     0  ...    6  391     19.2  396.90  14.33
172  505  0.10959   0.0  11.93     0  ...    1  273     21.0  393.45   6.48

[173 rows x 14 columns]

```
:::


Inspect the loaded datasets as pandas DataFrames.


::: {.sourceClojure}
```clojure
(def X-train (py. train-data drop "medv" :axis 1))
```
:::



::: {.sourceClojure}
```clojure
(def y-train (py. train-data "get" "medv"))
```
:::


We separate the features (X-train) and target variable (y-train) from the training data.

***2) Perform Ridge regression on the train data***


::: {.sourceClojure}
```clojure
(def ridge-model (linear-model/Ridge))
```
:::



::: {.sourceClojure}
```clojure
(def param-grid {"alpha" [150 160 165 170 175 180 185 190 195 200]})
```
:::



::: {.sourceClojure}
```clojure
(def cv-ridge (model-selection/GridSearchCV ridge-model param-grid :cv 5))
```
:::


Here we set up the Ridge regression model and define a parameter grid for alpha values. We then create a GridSearchCV object for 5-fold cross-validation.


::: {.sourceClojure}
```clojure
(py. cv-ridge fit X-train y-train)
```
:::



::: {.printedClojure}
```clojure
GridSearchCV(cv=5, estimator=Ridge(),
             param_grid={'alpha': [150, 160, 165, 170, 175, 180, 185, 190, 195,
                                   200]})

```
:::


This line fits the Ridge regression model using GridSearchCV on the training data.


::: {.sourceClojure}
```clojure
(answer (str "Best parameters:" (py.- cv-ridge best_params_)))
```
:::


> <span style="color: black; font-size: 1.5em;">**Best parameters:{'alpha': 180}**</span>


::: {.sourceClojure}
```clojure
(answer (str "Best CV score:" (py.- cv-ridge best_score_)))
```
:::


> <span style="color: black; font-size: 1.5em;">**Best CV score:0.17820256573782292**</span>

### K-fold Cross Validation Procedure:

1) The training data is divided into 5 equal parts (folds).
2) For each fold:

   a) That fold is treated as a validation set.
   b) The model is trained on the remaining 4 folds.
   c) The model's performance is evaluated on the validation fold.

3) This process is repeated 5 times, with each fold serving as the validation set once.
4) The average performance across all 5 validations is used as the cross-validation score.
5) This entire procedure is repeated for each hyperparameter combination (different alpha values).
6) The hyperparameters that yield the best average performance are selected.

 This method provides a robust estimate of the model's performance and helps in selecting the best hyperparameters, reducing the risk of overfitting.

Below, we print the best model's intercept and coefficients, and construct a human-readable equation for the Ridge regression model.


::: {.sourceClojure}
```clojure
(let [best-model (py.- cv-ridge best_estimator_)
      intercept (py.- best-model intercept_)
      coefficients (py.- best-model coef_)
      feature-names (py.- X-train columns)]
  (answer
   (str "Ridge Regression Equation:\n"
        "$medv = "
        (format "%.4f" intercept)
        (apply str
               (map (fn [name coef]
                      (format " %s %.4f * %s"
                              (if (pos? coef) "+" "-")
                              (Math/abs coef)
                              name))
                    feature-names
                    coefficients)) "$")))
```
:::


> <span style="color: black; font-size: 1.5em;">**Ridge Regression Equation:
$medv = 39.8963 - 0.0023 * ID - 0.0490 * crim + 0.0578 * zn + 0.0043 * indus + 0.3847 * chas - 0.0680 * nox + 1.3832 * rm + 0.0129 * age - 0.9845 * dis + 0.3697 * rad - 0.0167 * tax - 0.7580 * ptratio + 0.0116 * black - 0.7889 * lstat$**</span>

***3) Justify the choice of K***

We chose $K=5$ for cross-validation as it provides a good balance between bias and variance. It's a common choice that works well for most datasets, offering reliable performance estimates without excessive computational cost.

***4) Test the model on the test data***


::: {.sourceClojure}
```clojure
(def best-ridge-model (py.- cv-ridge best_estimator_))
```
:::


Here we extract the best Ridge regression model from the GridSearchCV results.


::: {.sourceClojure}
```clojure
(def y-pred (py. best-ridge-model predict test-data))
```
:::


`y-pred` is the predictions on the entire test dataset using the best Ridge regression model.

***5) Analyze the importance of each feature and justify your results in the report***


::: {.sourceClojure}
```clojure
(def feature-importance (py.- best-ridge-model coef_))
```
:::



::: {.sourceClojure}
```clojure
(def feature-names (py.- X-train columns))
```
:::


These lines extract the feature coefficients (which represent feature importance in linear models) and feature names from the best model and training data, respectively.


::: {.sourceClojure}
```clojure
(let [feature-names (vec feature-names)
      feature-importance (vec feature-importance)
      data (tc/dataset {:vars feature-names
                        :importance feature-importance})
      sorted (tc/order-by data :importance :desc)]
  (-> sorted
      (haclo/layer-bar
       {:=y :vars :=x :importance       ;order-by??
        :=title "Feature Importances"})))
```
:::



```{=html}
<div><script>vegaEmbed(document.currentScript.parentElement, {"encoding":{"x":{"field":"x","type":"nominal"},"y":{"field":"y","type":"nominal"}},"usermeta":{"embedOptions":{"renderer":"svg"}},"width":400,"background":"floralwhite","layer":[{"mark":{"type":"bar","tooltip":true},"encoding":{"x":{"field":"importance","type":"quantitative"},"y":{"field":"vars","type":"nominal"}},"data":{"values":"vars,importance\nrm,1.3832310453542689\nchas,0.3846674322920127\nrad,0.3697238002007228\nzn,0.05778196417719924\nage,0.012925702145975593\nblack,0.011621283571712876\nindus,0.004296184605311984\nID,-0.002269139274071281\ntax,-0.016673829625451803\ncrim,-0.049041726989618964\nnox,-0.06800806864610484\nptratio,-0.7579591504170874\nlstat,-0.7888725000106315\ndis,-0.984478760041314\n","format":{"type":"csv"}}}],"height":300,"data":{"url":"assignments.hw2.q2_files\/0.csv","format":{"type":"csv"}}});</script></div>
```


The barplot shows the regressor coefficients, which are proportional to the feature importances. The plot is generated using the Hanami plotting library.


::: {.sourceClojure}
```clojure
(answer
 (str "Feature importances:\n"
      (clojure.string/join "\n"
                           (for [[feature importance] (map vector feature-names feature-importance)]
                             (format "%-20s : %.4f ;  " feature importance)))))
```
:::


> <span style="color: black; font-size: 1.5em;">**Feature importances:
ID                   : -0.0023 ;  
crim                 : -0.0490 ;  
zn                   : 0.0578 ;  
indus                : 0.0043 ;  
chas                 : 0.3847 ;  
nox                  : -0.0680 ;  
rm                   : 1.3832 ;  
age                  : 0.0129 ;  
dis                  : -0.9845 ;  
rad                  : 0.3697 ;  
tax                  : -0.0167 ;  
ptratio              : -0.7580 ;  
black                : 0.0116 ;  
lstat                : -0.7889 ;  **</span>

This generates a formatted string output of all feature importances, aligning feature names and their corresponding importance values.

Sort features by absolute importance


::: {.sourceClojure}
```clojure
(def sorted-features
  (->> (map vector feature-names feature-importance)
       (sort-by #(Math/abs (second %)))
       reverse))
```
:::


This code sorts the features by the absolute value of their importance (coefficient magnitude) in descending order. This is useful because both large positive and large negative coefficients indicate important features in linear models.

Finally, this code outputs a formatted string of the top 5 most important features based on the absolute value of their coefficients. This provides a quick summary of which features have the largest impact on the model's predictions.


::: {.sourceClojure}
```clojure
(answer
 (str "\nTop 5 most important features:\n"
      (clojure.string/join "\n"
                           (for [[feature importance] (take 5 sorted-features)]
                             (format "%-20s : %.4f ;  " feature (float importance))))))
```
:::


> <span style="color: black; font-size: 1.5em;">**
Top 5 most important features:
rm                   : 1.3832 ;  
dis                  : -0.9845 ;  
lstat                : -0.7889 ;  
ptratio              : -0.7580 ;  
chas                 : 0.3847 ;  **</span>

### Explanation of feature importances:
 
1. The most important features are those with the largest absolute coefficient values.
2. Positive coefficients indicate that an increase in the feature leads to an increase in the predicted house price, while negative coefficients indicate the opposite.
3. The magnitude of the coefficient represents the feature's relative importance in predicting the house price.
4. Features with near-zero coefficients have little impact on the prediction.

This analysis helps us understand which factors most strongly influence house prices in the Boston housing dataset.


```{=html}
<div style="background-color:grey;height:2px;width:100%;"></div>
```



```{=html}
<div><pre><small><small>source: <a href="https://github.com/adabwana/f24-cs7300-hw2/blob/master/src/assignments/hw2/q2.clj">src/assignments/hw2/q2.clj</a></small></small></pre></div>
```
