{"title":"Question 3","markdown":{"yaml":{"format":{"html":{"toc":true,"toc-depth":3,"theme":"cosmo","number-sections":false,"output-file":"assignments.hw2.q3.html"}},"fontsize":"0.9em","code-block-background":true,"include-in-header":{"text":"<link rel = \"icon\" href = \"data:,\" />"},"toc-title-numbers":false,"number-depth":0},"headingText":"Question 3","containsRefs":false,"markdown":"\n<style></style><style>.printedClojure .sourceCode {\n  background-color: transparent;\n  border-style: none;\n}\n</style><style>.clay-limit-image-width .clay-image {max-width: 100%}\n.clay-side-by-side .sourceCode {margin: 0}\n.clay-side-by-side {margin: 1em 0}\n</style>\n<script src=\"assignments.hw2.q3_files/md-default0.js\" type=\"text/javascript\"></script><script src=\"assignments.hw2.q3_files/md-default1.js\" type=\"text/javascript\"></script>\n\n::: {.sourceClojure}\n```clojure\n(ns assignments.hw2.q3\n  (:require\n   [assignments.hw2.utils :refer :all]\n   [tablecloth.api :as tc]\n   [fastmath.core :as m]\n   [fastmath.stats :as s]))\n```\n:::\n\n\n---\n\n#### *Q3: Writing Questions (20 points)*\n\n***1) What is the curse of dimensionality and list three ways to avoid it. (6 points)***\n\n### Curse of Dimensionality\n\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. As the number of features or dimensions grows, the amount of data needed to generalize accurately grows exponentially. This curse makes it challenging for machine learning algorithms to be effective in high dimensions.\n\n#### Key aspects of the curse of dimensionality:\n\n##### 1. Sparsity:\n   Data becomes sparse in high dimensions, making it difficult to find patterns.\n\n##### 2. Distance Concentration: \n   Distances between points become less meaningful as dimensions increase.\n\n##### 3. Model Complexity: \n   The number of parameters in models often increases exponentially with dimensions.\n\n#### Three ways to mitigate the curse of dimensionality:\n\n##### 1. Dimensionality Reduction: \n   - Use techniques like PCA, t-SNE, or autoencoders to reduce the number of features while preserving important information.\n   - This helps in visualizing high-dimensional data and can improve model performance.\n\n##### 2. Feature Selection:\n   - Choose only the most relevant features for your model using methods like Lasso, Ridge regression, or tree-based feature importance.\n   - This reduces noise and helps focus on the most informative aspects of the data.\n\n##### 3. Regularization:\n   - Apply techniques like $L^1$ (Lasso) or $L^2$ (Ridge) regularization to prevent overfitting in high-dimensional spaces.\n   - This constrains model complexity and can lead to better generalization.\n\n#### Additional Strategies \n  We could also collect more data, use domain knowledge to guide feature engineering, and employ algorithms that are less sensitive to high dimensionality, such as decision trees or random forests.\n\n***2) Explain the differences between Ridge penalty and Lasso penalty. (6 points)***\n\n### Ridge vs. Lasso\n\nRidge ($L^2$) and Lasso ($L^1$) penalties are both regularization techniques used in linear models to prevent overfitting, but they differ in their approach and effects:\n\n#### 1. Mathematical Formulation:\n  - ***Ridge:*** Adds the sum of squared coefficients (L2 norm) to the loss function. Penalty term: $$λ_2 * Σ(β_i^2)$$\n  - ***Lasso:*** Adds the sum of absolute values of coefficients (L1 norm) to the loss function. Penalty term: $$λ_1 * Σ|β_i|$$\n\n#### 2. Effect on Coefficients:\n  - ***Ridge:*** Shrinks all coefficients towards zero, but rarely makes them exactly zero.\n  - ***Lasso:*** Can shrink coefficients to exactly zero, effectively performing feature selection.\n\n#### 3. Geometry:\n  - ***Ridge:*** Creates a circular (in 2D) or spherical (in higher dimensions) constraint region.\n  - ***Lasso:*** Creates a diamond-shaped (in 2D) or pyramidal (in higher dimensions) constraint region.\n\n#### 4. Solution Characteristics:\n  - ***Ridge:*** Always has a unique solution due to the strictly convex nature of its objective function.\n  - ***Lasso:*** May have multiple solutions when features are highly correlated.\n\n#### 5. Feature Selection:\n  - ***Ridge:*** Keeps all features but reduces their impact.\n  - ***Lasso:*** Can completely eliminate less important features, leading to sparse models.\n\n#### 6. Handling Correlated Features:\n  - ***Ridge:*** Tends to shrink coefficients of correlated features towards each other.\n  - ***Lasso:*** Tends to pick one of the correlated features and ignore the others.\n\nIn practice, the choice between Ridge and Lasso depends on the specific problem. Ridge is often preferred when all features are potentially relevant, while Lasso is useful when feature selection is desired or when dealing with high-dimensional data with many irrelevant features)\n\n***3) Demonstrate your insights for both direct and iterative methods for optimization. (8 points)***\n\n\nDirect and iterative methods are two fundamental approaches to optimization in machine learning and numerical computing:\n\n### Direct and Iterative Methods:\n     \n#### **Direct Methods:**\n\n##### 1. Characteristics:\n   - Attempt to solve the problem in a finite number of steps.\n   - Often involve matrix operations or analytical solutions.\n   - Examples: Gaussian elimination, LU decomposition, closed-form solutions.\n\n##### 2. Advantages and Disadvantages:\n\n| Advantages | Disadvantages |\n|------------|---------------|\n| Can provide exact solutions for certain problems | May be computationally expensive or infeasible for large-scale problems |\n| Often faster for small to medium-sized problems | Can be sensitive to numerical errors in floating-point arithmetic |\n| No need to specify convergence criteria or iteration limits | Not always available for complex, non-linear problems |\n\n##### 3. Use Cases:\n   - Solving systems of linear equations.\n   - Finding eigenvalues and eigenvectors.\n   - Least squares problems with closed-form solutions.\n\n#### **Iterative Methods:**\n\n##### 1. Characteristics:\n   - Start with an initial guess and progressively refine the solution.\n   - Involve repeated application of an algorithm until convergence.\n   - Examples: Gradient descent, Newton's method, conjugate gradient.\n\n##### 2. Advantages and Disadvantages:\n\n| Advantages | Disadvantages |\n|------------|---------------|\n| Can handle large-scale and complex optimization problems | May require many iterations to converge |\n| Often more memory-efficient than direct methods | Convergence is not always guaranteed and can depend on initial conditions |\n| Can provide approximate solutions quickly, even if not run to full convergence | Need to specify stopping criteria (e.g., tolerance, maximum iterations) |\n\n##### 3. Use Cases:\n   - Training of neural networks and other machine learning models.\n   - Solving large sparse systems of equations.\n   - Optimization problems where direct methods are infeasible.\n\n#### **Insights:**\n\n##### 1. Problem-Dependent Choice: \n   The choice between direct and iterative methods often depends on the specific problem, its size, and structure.\n\n##### 2. Hybrid Approaches:\n   Some advanced algorithms combine aspects of both, like using direct methods as preconditioners for iterative methods.\n\n##### 3. Scalability Considerations:\n   As problem sizes grow, iterative methods often become the only feasible option due to memory and computational constraints.\n\n##### 4. Accuracy vs. Speed Trade-off:\n   Direct methods can provide highly accurate solutions but may be slow for large problems. Iterative methods can often provide good approximate solutions quickly.\n\n##### 5. Robustness:\n   Iterative methods can be more robust to certain types of numerical errors and can handle ill-conditioned problems better in some cases.\n\n##### 6. Adaptability:\n   Iterative methods are often more adaptable to changes in the problem structure or constraints during the optimization process.\n\nIn machine learning, iterative methods are prevalent due to the large-scale nature of many problems and the need for flexibility in handling various loss functions and model architectures. However, understanding both approaches is crucial for selecting the most appropriate method for a given optimization task\n\n\n```{=html}\n<div style=\"background-color:grey;height:2px;width:100%;\"></div>\n```\n\n\n\n```{=html}\n<div></div>\n```\n","srcMarkdownNoYaml":"\n<style></style><style>.printedClojure .sourceCode {\n  background-color: transparent;\n  border-style: none;\n}\n</style><style>.clay-limit-image-width .clay-image {max-width: 100%}\n.clay-side-by-side .sourceCode {margin: 0}\n.clay-side-by-side {margin: 1em 0}\n</style>\n<script src=\"assignments.hw2.q3_files/md-default0.js\" type=\"text/javascript\"></script><script src=\"assignments.hw2.q3_files/md-default1.js\" type=\"text/javascript\"></script>\n\n::: {.sourceClojure}\n```clojure\n(ns assignments.hw2.q3\n  (:require\n   [assignments.hw2.utils :refer :all]\n   [tablecloth.api :as tc]\n   [fastmath.core :as m]\n   [fastmath.stats :as s]))\n```\n:::\n\n\n## Question 3\n---\n\n#### *Q3: Writing Questions (20 points)*\n\n***1) What is the curse of dimensionality and list three ways to avoid it. (6 points)***\n\n### Curse of Dimensionality\n\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. As the number of features or dimensions grows, the amount of data needed to generalize accurately grows exponentially. This curse makes it challenging for machine learning algorithms to be effective in high dimensions.\n\n#### Key aspects of the curse of dimensionality:\n\n##### 1. Sparsity:\n   Data becomes sparse in high dimensions, making it difficult to find patterns.\n\n##### 2. Distance Concentration: \n   Distances between points become less meaningful as dimensions increase.\n\n##### 3. Model Complexity: \n   The number of parameters in models often increases exponentially with dimensions.\n\n#### Three ways to mitigate the curse of dimensionality:\n\n##### 1. Dimensionality Reduction: \n   - Use techniques like PCA, t-SNE, or autoencoders to reduce the number of features while preserving important information.\n   - This helps in visualizing high-dimensional data and can improve model performance.\n\n##### 2. Feature Selection:\n   - Choose only the most relevant features for your model using methods like Lasso, Ridge regression, or tree-based feature importance.\n   - This reduces noise and helps focus on the most informative aspects of the data.\n\n##### 3. Regularization:\n   - Apply techniques like $L^1$ (Lasso) or $L^2$ (Ridge) regularization to prevent overfitting in high-dimensional spaces.\n   - This constrains model complexity and can lead to better generalization.\n\n#### Additional Strategies \n  We could also collect more data, use domain knowledge to guide feature engineering, and employ algorithms that are less sensitive to high dimensionality, such as decision trees or random forests.\n\n***2) Explain the differences between Ridge penalty and Lasso penalty. (6 points)***\n\n### Ridge vs. Lasso\n\nRidge ($L^2$) and Lasso ($L^1$) penalties are both regularization techniques used in linear models to prevent overfitting, but they differ in their approach and effects:\n\n#### 1. Mathematical Formulation:\n  - ***Ridge:*** Adds the sum of squared coefficients (L2 norm) to the loss function. Penalty term: $$λ_2 * Σ(β_i^2)$$\n  - ***Lasso:*** Adds the sum of absolute values of coefficients (L1 norm) to the loss function. Penalty term: $$λ_1 * Σ|β_i|$$\n\n#### 2. Effect on Coefficients:\n  - ***Ridge:*** Shrinks all coefficients towards zero, but rarely makes them exactly zero.\n  - ***Lasso:*** Can shrink coefficients to exactly zero, effectively performing feature selection.\n\n#### 3. Geometry:\n  - ***Ridge:*** Creates a circular (in 2D) or spherical (in higher dimensions) constraint region.\n  - ***Lasso:*** Creates a diamond-shaped (in 2D) or pyramidal (in higher dimensions) constraint region.\n\n#### 4. Solution Characteristics:\n  - ***Ridge:*** Always has a unique solution due to the strictly convex nature of its objective function.\n  - ***Lasso:*** May have multiple solutions when features are highly correlated.\n\n#### 5. Feature Selection:\n  - ***Ridge:*** Keeps all features but reduces their impact.\n  - ***Lasso:*** Can completely eliminate less important features, leading to sparse models.\n\n#### 6. Handling Correlated Features:\n  - ***Ridge:*** Tends to shrink coefficients of correlated features towards each other.\n  - ***Lasso:*** Tends to pick one of the correlated features and ignore the others.\n\nIn practice, the choice between Ridge and Lasso depends on the specific problem. Ridge is often preferred when all features are potentially relevant, while Lasso is useful when feature selection is desired or when dealing with high-dimensional data with many irrelevant features)\n\n***3) Demonstrate your insights for both direct and iterative methods for optimization. (8 points)***\n\n\nDirect and iterative methods are two fundamental approaches to optimization in machine learning and numerical computing:\n\n### Direct and Iterative Methods:\n     \n#### **Direct Methods:**\n\n##### 1. Characteristics:\n   - Attempt to solve the problem in a finite number of steps.\n   - Often involve matrix operations or analytical solutions.\n   - Examples: Gaussian elimination, LU decomposition, closed-form solutions.\n\n##### 2. Advantages and Disadvantages:\n\n| Advantages | Disadvantages |\n|------------|---------------|\n| Can provide exact solutions for certain problems | May be computationally expensive or infeasible for large-scale problems |\n| Often faster for small to medium-sized problems | Can be sensitive to numerical errors in floating-point arithmetic |\n| No need to specify convergence criteria or iteration limits | Not always available for complex, non-linear problems |\n\n##### 3. Use Cases:\n   - Solving systems of linear equations.\n   - Finding eigenvalues and eigenvectors.\n   - Least squares problems with closed-form solutions.\n\n#### **Iterative Methods:**\n\n##### 1. Characteristics:\n   - Start with an initial guess and progressively refine the solution.\n   - Involve repeated application of an algorithm until convergence.\n   - Examples: Gradient descent, Newton's method, conjugate gradient.\n\n##### 2. Advantages and Disadvantages:\n\n| Advantages | Disadvantages |\n|------------|---------------|\n| Can handle large-scale and complex optimization problems | May require many iterations to converge |\n| Often more memory-efficient than direct methods | Convergence is not always guaranteed and can depend on initial conditions |\n| Can provide approximate solutions quickly, even if not run to full convergence | Need to specify stopping criteria (e.g., tolerance, maximum iterations) |\n\n##### 3. Use Cases:\n   - Training of neural networks and other machine learning models.\n   - Solving large sparse systems of equations.\n   - Optimization problems where direct methods are infeasible.\n\n#### **Insights:**\n\n##### 1. Problem-Dependent Choice: \n   The choice between direct and iterative methods often depends on the specific problem, its size, and structure.\n\n##### 2. Hybrid Approaches:\n   Some advanced algorithms combine aspects of both, like using direct methods as preconditioners for iterative methods.\n\n##### 3. Scalability Considerations:\n   As problem sizes grow, iterative methods often become the only feasible option due to memory and computational constraints.\n\n##### 4. Accuracy vs. Speed Trade-off:\n   Direct methods can provide highly accurate solutions but may be slow for large problems. Iterative methods can often provide good approximate solutions quickly.\n\n##### 5. Robustness:\n   Iterative methods can be more robust to certain types of numerical errors and can handle ill-conditioned problems better in some cases.\n\n##### 6. Adaptability:\n   Iterative methods are often more adaptable to changes in the problem structure or constraints during the optimization process.\n\nIn machine learning, iterative methods are prevalent due to the large-scale nature of many problems and the need for flexibility in handling various loss functions and model architectures. However, understanding both approaches is crucial for selecting the most appropriate method for a given optimization task\n\n\n```{=html}\n<div style=\"background-color:grey;height:2px;width:100%;\"></div>\n```\n\n\n\n```{=html}\n<div></div>\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"include-in-header":{"text":"<link rel = \"icon\" href = \"data:,\" />"},"output-file":"assignments.hw2.q3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","fontsize":"0.9em","code-block-background":true,"toc-title-numbers":false,"number-depth":0},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}